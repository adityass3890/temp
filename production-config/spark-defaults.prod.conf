# Production Spark Configuration for DLM Migration
# Copy this file to $SPARK_HOME/conf/spark-defaults.conf

# Memory Settings - Optimize for production workload
spark.driver.memory              8g
spark.driver.maxResultSize       4g
spark.executor.memory            12g
spark.executor.memoryOverhead    2048m
spark.executor.memoryFraction    0.8

# Core Settings - Adjust based on cluster resources
spark.executor.cores             6
spark.executor.instances         16
spark.default.parallelism        96
spark.sql.shuffle.partitions     400

# Dynamic Allocation for resource optimization
spark.dynamicAllocation.enabled  true
spark.dynamicAllocation.minExecutors 4
spark.dynamicAllocation.maxExecutors 32
spark.dynamicAllocation.initialExecutors 8
spark.dynamicAllocation.executorIdleTimeout 60s
spark.dynamicAllocation.cachedExecutorIdleTimeout 300s

# Network and Timeout Settings
spark.network.timeout            600s
spark.executor.heartbeatInterval 30s
spark.sql.broadcastTimeout      300

# Serialization for better performance
spark.serializer                 org.apache.spark.serializer.KryoSerializer
spark.kryo.unsafe                true
spark.kryo.registrationRequired  false

# SQL Adaptive Query Execution
spark.sql.adaptive.enabled       true
spark.sql.adaptive.coalescePartitions.enabled true
spark.sql.adaptive.skewJoin.enabled true
spark.sql.adaptive.localShuffleReader.enabled true
spark.sql.autoBroadcastJoinThreshold 67108864

# Checkpointing and Recovery
spark.sql.recovery.checkpointDir file:///app/tetra/DLM/checkpoints
spark.sql.streaming.checkpointLocation file:///app/tetra/DLM/streaming-checkpoints

# History Server Configuration
spark.eventLog.enabled           true
spark.eventLog.dir               file:///app/tetra/DLM/spark-events
spark.eventLog.compress          true
spark.history.fs.logDirectory    file:///app/tetra/DLM/spark-events
spark.history.ui.port            18080

# Security Settings
spark.authenticate               true
spark.network.crypto.enabled     true
spark.io.encryption.enabled      true
spark.sql.execution.arrow.pyspark.enabled false

# Cassandra Connector Settings
spark.cassandra.connection.timeout_ms        30000
spark.cassandra.read.timeout_ms              120000
spark.cassandra.connection.reconnection_delay_ms.max 30000
spark.cassandra.connection.reconnection_delay_ms.min 1000
spark.cassandra.connection.keep_alive_ms     300000

# Oracle JDBC Settings
spark.sql.execution.arrow.maxRecordsPerBatch 10000
spark.serializer.objectStreamReset          100

# Optimization for large datasets
spark.sql.files.maxPartitionBytes           268435456
spark.sql.files.openCostInBytes             4194304
spark.sql.adaptive.coalescePartitions.minPartitionNum 1
spark.sql.adaptive.advisoryPartitionSizeInBytes 67108864

# Garbage Collection Settings
spark.executor.extraJavaOptions  -XX:+UseG1GC -XX:MaxGCPauseMillis=200 -XX:G1HeapRegionSize=32m -XX:+UnlockExperimentalVMOptions -XX:+UseJVMCICompiler
spark.driver.extraJavaOptions    -XX:+UseG1GC -XX:MaxGCPauseMillis=200 -XX:G1HeapRegionSize=32m

# Staging and temporary directories
spark.local.dir                  /tmp/spark-local,/app/tetra/DLM/tmp/spark-local
spark.sql.warehouse.dir          /app/tetra/DLM/spark-warehouse

# Monitoring and Metrics
spark.sql.streaming.metricsEnabled true
spark.metrics.conf               /app/tetra/DLM/config/metrics.properties

# Application specific
spark.app.name                   DLM-Migration-Production
spark.submit.deployMode          cluster
spark.master                     yarn